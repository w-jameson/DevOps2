[
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "from palmerpenguins import penguins\nfrom pandas import get_dummies\nimport duckdb\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n#import matplotlib.pyplot as plt"
  },
  {
    "objectID": "model.html#import-packages",
    "href": "model.html#import-packages",
    "title": "Model",
    "section": "",
    "text": "from palmerpenguins import penguins\nfrom pandas import get_dummies\nimport duckdb\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n#import matplotlib.pyplot as plt"
  },
  {
    "objectID": "model.html#get-data",
    "href": "model.html#get-data",
    "title": "Model",
    "section": "Get Data",
    "text": "Get Data\n\n#con = duckdb.connect('my-db.duckdb')\n#df = penguins.load_penguins()\n#con.execute('CREATE TABLE penguins AS SELECT * FROM df')\n#con.close()\n\n\ncon = duckdb.connect('my-db.duckdb')\ndf = con.execute(\"SELECT * FROM penguins\").fetchdf().dropna()"
  },
  {
    "objectID": "model.html#establish-predictors-and-traintest-split",
    "href": "model.html#establish-predictors-and-traintest-split",
    "title": "Model",
    "section": "Establish Predictors and train/test split",
    "text": "Establish Predictors and train/test split\nThe parameters for the model are bill_length_mm, species, and sex, with the model trying to predict the body_mass_g, or mass in grams for the penguin based on the parameters.\n\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
  },
  {
    "objectID": "model.html#models-overview",
    "href": "model.html#models-overview",
    "title": "Model",
    "section": "Models Overview",
    "text": "Models Overview\nTo best predict the mass in grams of penguins using the Palmer’s Penguins Dataset, the following regression methods are used:\n\nMultiple Linear Regression\nRegression Tree\nRandom Forest\nBoosting\nRidge Regression\nLasso Regression\nPolynomial Regression\n\nModel performance will be compared using RMSE and the regression methods are performed using tools from the sklearn package.\nAdditionally, to determine the best performance for Random Forest, Boosting, Ridge Regression, Lasso Regression, and Polynomial Regression, hyperparameter tuning will be performed and compared to the base model. Tuning will be performed using the grid search method from sklearn’s GridSearchCV. Below is the function used for scoring parameters for tuning based on RMSE values.\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nrmse_scorer = make_scorer(rmse, greater_is_better=False)  \n# Negative because GridSearchCV seeks to maximize the score\n\nThis function combined with the grid search setups tune the hyperparameters to maximize the negative RMSE- which in practices attempts to find the minimum RMSE."
  },
  {
    "objectID": "model.html#linear-regression",
    "href": "model.html#linear-regression",
    "title": "Model",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\ny_pred_linear = linear_model.predict(X_test)\nlinear_rmse = np.sqrt(mean_squared_error(y_test, y_pred_linear))\nprint(f\"Linear Regression RMSE: {linear_rmse}\")\n\nLinear Regression RMSE: 290.09581312213606\n\n\nThe RMSE is calculated to be 290.096, which will be used to compare against the subsequent models."
  },
  {
    "objectID": "model.html#decision-tree",
    "href": "model.html#decision-tree",
    "title": "Model",
    "section": "Decision Tree",
    "text": "Decision Tree\n\ntree_model = DecisionTreeRegressor(random_state=1)\ntree_model.fit(X_train, y_train)\ny_pred_tree = tree_model.predict(X_test)\ndecision_tree_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tree))\nprint(f\"Decision Tree RMSE: {decision_tree_rmse}\")\n\nDecision Tree RMSE: 457.5074988737395\n\n\nThe calculated RMSE for this of 457.507 is significantly higher than that of the Linear Regression model, implying that it is a less accurate model overall."
  },
  {
    "objectID": "model.html#random-forest",
    "href": "model.html#random-forest",
    "title": "Model",
    "section": "Random Forest",
    "text": "Random Forest\nFirst, we will look at the un-tuned model seen below.\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(X_train, y_train)\ny_pred_forest = forest_model.predict(X_test)\nrf_untuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_forest))\nprint(f\"Random Forest RMSE: {rf_untuned_rmse}\")\n\nRandom Forest RMSE: 381.64031610614956\n\n\nThe RMSE was calculated to be 381.634, which is an improvement over the single decision tree but falls short of Linear Regression.\nNow, we will use hyperparameter tuning to determine the best parameters to minimize RMSE and the calculated RMSE value using those parameters.\n\n# Setup the parameter grid\nparam_grid_rf = {\n    'n_estimators': [100, 200],\n    'max_features': ['sqrt', 'log2'],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid Search to find the best parameters\ngrid_rf = GridSearchCV(estimator=RandomForestRegressor(random_state=1), \n                       param_grid=param_grid_rf, \n                       scoring=rmse_scorer, \n                       cv=10, \n                       verbose=1)\ngrid_rf.fit(X_train, y_train)\n\n# Evaluate on test data\ny_pred_rf = grid_rf.predict(X_test)\nrf_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(f\"Best parameters for Random Forest: {grid_rf.best_params_}\")\nprint(f\"Test RMSE for Random Forest: {rf_tuned_rmse}\")\n\nFitting 10 folds for each of 72 candidates, totalling 720 fits\nBest parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\nTest RMSE for Random Forest: 311.0271134033527\n\n\nFrom tuning the model, the test RMSE was reduced to 311.027, which is still significantly higher than that of the Linear Regression model."
  },
  {
    "objectID": "model.html#boosting",
    "href": "model.html#boosting",
    "title": "Model",
    "section": "Boosting",
    "text": "Boosting\nOnce again, we will be looking at the un-tuned Boosting model as a baseline.\n\nboosting_model = GradientBoostingRegressor(random_state=1)\nboosting_model.fit(X_train, y_train)\ny_pred_boosting = boosting_model.predict(X_test)\nboost_untuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_boosting))\nprint(f\"Boosting RMSE: {boost_untuned_rmse}\")\n\nBoosting RMSE: 334.4818790948876\n\n\nThis gives an RMSE of 334.482, which is an improvement over the un-tuned Random Forest RMSE.\nSimilarly to Random Forest, tuning will be performed to see if model performance increases.\n\n# Setup the parameter grid\nparam_grid_gb = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.8, 1.0],\n    'max_depth': [3, 4, 5]\n}\n\n# Grid Search to find the best parameters\ngrid_gb = GridSearchCV(estimator=GradientBoostingRegressor(random_state=1), \n                       param_grid=param_grid_gb, \n                       scoring=rmse_scorer, \n                       cv=10, \n                       verbose=1)\ngrid_gb.fit(X_train, y_train)\n\n# Evaluate on test data\ny_pred_gb = grid_gb.predict(X_test)\nboost_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\nprint(f\"Best parameters for Gradient Boosting: {grid_gb.best_params_}\")\nprint(f\"Test RMSE for Gradient Boosting: {boost_tuned_rmse}\")\n\nFitting 10 folds for each of 36 candidates, totalling 360 fits\nBest parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\nTest RMSE for Gradient Boosting: 331.12675171300873\n\n\nThis gives a tuned test RMSE of 331.127, which is an improvement over the un-tuned model, but still falls well short of that of the linear regression model."
  },
  {
    "objectID": "model.html#ridge-regression",
    "href": "model.html#ridge-regression",
    "title": "Model",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nThe un-tuned Ridge Regression model is depicted below.\n\nridge_model = Ridge(random_state=1)\nridge_model.fit(X_train, y_train)\ny_pred_ridge = ridge_model.predict(X_test)\nridge_untuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\nprint(f\"Ridge Regression RMSE: {ridge_untuned_rmse}\")\n\nRidge Regression RMSE: 290.7129581931783\n\n\nThe RMSE for this model is 290.713, which is only slightly higher than that of Linear Regression, which makes sense considering both are built using least squares. Let’s check if the tuned model improves this.\n\n# Define the pipeline\nridge_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('ridge', Ridge())\n])\n\n# Setup the parameter grid\nparam_grid_ridge = {\n    'ridge__alpha': [0.01, 0.1, 1, 10, 100]\n}\n\n# Grid Search to find the best parameters\ngrid_ridge = GridSearchCV(ridge_pipeline, param_grid_ridge, cv=10, scoring=rmse_scorer, verbose=1)\ngrid_ridge.fit(X_train, y_train)\n\n# Evaluate on test data\ny_pred_ridge = grid_ridge.predict(X_test)\nridge_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\nprint(f\"Best parameters for Ridge: {grid_ridge.best_params_}\")\nprint(f\"Test RMSE for Ridge: {ridge_tuned_rmse}\")\n\nFitting 10 folds for each of 5 candidates, totalling 50 fits\nBest parameters for Ridge: {'ridge__alpha': 1}\nTest RMSE for Ridge: 290.2326493952725\n\n\nThe test RMSE of 290.233 was marginally better than that of the un-tuned model and still slightly worse than Linear Regression but with additionally model complexity."
  },
  {
    "objectID": "model.html#lasso-regression",
    "href": "model.html#lasso-regression",
    "title": "Model",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nThe un-tuned Lasso model is shown below.\n\nlasso_model = Lasso(random_state=1)\nlasso_model.fit(X_train, y_train)\ny_pred_lasso = lasso_model.predict(X_test)\nlasso_untuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\nprint(f\"Lasso Regression RMSE: {lasso_untuned_rmse}\")\n\nLasso Regression RMSE: 290.62380428183945\n\n\nThe Lasso model performs similarly to the Ridge Regression mode, as to be expected in this scenario. The RMSE is 290.624. Tuning is conducted in the cell below.\n\n# Define the pipeline\nlasso_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lasso', Lasso(max_iter=10000))\n])\n\n# Setup the parameter grid\nparam_grid_lasso = {\n    'lasso__alpha': [0.01, 0.1, 1, 10, 100]\n}\n\n# Grid Search to find the best parameters\ngrid_lasso = GridSearchCV(lasso_pipeline, param_grid_lasso, cv=10, scoring=rmse_scorer, verbose=1)\ngrid_lasso.fit(X_train, y_train)\n\n# Evaluate on test data\ny_pred_lasso = grid_lasso.predict(X_test)\nlasso_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\nprint(f\"Best parameters for Lasso: {grid_lasso.best_params_}\")\nprint(f\"Test RMSE for Lasso: {lasso_tuned_rmse}\")\n\nFitting 10 folds for each of 5 candidates, totalling 50 fits\nBest parameters for Lasso: {'lasso__alpha': 1}\nTest RMSE for Lasso: 290.4642234879255\n\n\nTuning the Lasso Regression model has little effect on the test RMSE, decreasing slightly to 290.464, which is still slightly higher than that of Linear Regression."
  },
  {
    "objectID": "model.html#polynomial-regression",
    "href": "model.html#polynomial-regression",
    "title": "Model",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nNow, we will perform un-tuned polynomial regression using degree 2 as a baseline.\n\n# Create a pipeline that includes polynomial feature creation, scaling, and Linear Regression\npipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),  # Generates polynomial and interaction features up to the second degree\n    ('scaler', StandardScaler()),            # Standardizes the features\n    ('linear', LinearRegression())           # Applies Linear Regression\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Use the trained pipeline to make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Calculate the Root Mean Squared Error (RMSE) for the predictions\npoly_untuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"RMSE: {poly_untuned_rmse}\")\n\nRMSE: 280.3679077361095\n\n\nThe test RMSE of 279.156 is the first improvement on the original Linear Regression test RMSE. Tuning of the Polynomial Model is shown below.\n\n# Define a pipeline that includes polynomial feature creation, scaling, and Linear Regression\npipeline = Pipeline([\n    ('poly', PolynomialFeatures()),\n    ('scaler', StandardScaler()),\n    ('linear', LinearRegression())\n])\n\n# Define the parameter grid to tune the polynomial degrees\nparam_grid = {\n    'poly__degree': [1, 2, 3, 4, 5, 6, 7, 8]  # Testing polynomial degrees from 1 to 5\n}\n\n# Create a GridSearchCV object to tune the degrees\ngrid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='neg_root_mean_squared_error', verbose=1)\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# Finding the best polynomial degree and corresponding model\nbest_degree = grid_search.best_params_['poly__degree']\nbest_model = grid_search.best_estimator_\n\n# Use the best model to make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Calculate the Root Mean Squared Error (RMSE) for the predictions on the test set\npoly_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"Best polynomial degree: {best_degree}\")\nprint(f\"Test RMSE: {poly_tuned_rmse}\")\n\nFitting 10 folds for each of 8 candidates, totalling 80 fits\nBest polynomial degree: 1\nTest RMSE: 290.0958131221363\n\n\nThe tuned model here performs worse than the un-tuned model, and almost exactly the same as the first Linear Regression model with a test RMSE of 290.096."
  },
  {
    "objectID": "model.html#model-comparison",
    "href": "model.html#model-comparison",
    "title": "Model",
    "section": "Model Comparison",
    "text": "Model Comparison\nThe test RMSE for each model is seen in the graph below."
  },
  {
    "objectID": "model.html#model-selection",
    "href": "model.html#model-selection",
    "title": "Model",
    "section": "Model Selection",
    "text": "Model Selection\n\nmodel = LinearRegression()\n\n# Step 2: Fit the model to the data\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()"
  },
  {
    "objectID": "model.html#vetiver-model-and-pinning",
    "href": "model.html#vetiver-model-and-pinning",
    "title": "Model",
    "section": "Vetiver Model and Pinning",
    "text": "Vetiver Model and Pinning\n\nfrom vetiver import VetiverModel\nv = VetiverModel(model, model_name='penguin_model', prototype_data=X)\n\n\nfrom vetiver import VetiverAPI\napp = VetiverAPI(v, check_prototype=True)\n\n\nimport pins\nfrom vetiver import vetiver_pin_write\nboard = pins.board_folder(\"data/model\", allow_pickle_read = True)\nvetiver_pin_write(board, v)\n#board.pin_write(model, \"penguin_model\", type = \"joblib\")\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\nWriting pin:\nName: 'penguin_model'\nVersion: 20240421T020042Z-565d5"
  },
  {
    "objectID": "model.html#running-the-site",
    "href": "model.html#running-the-site",
    "title": "Model",
    "section": "Running the Site",
    "text": "Running the Site\n\n#app.run(port = 8080)\n\n\n#import requests\n#req_data = {\n#  \"bill_length_mm\": 0,\n#  \"species_Chinstrap\": False,\n#  \"species_Gentoo\": False,\n#  \"sex_male\": False\n#}\n\n\n#req = requests.post('http://127.0.0.1:8080/handler_predict', json = [req_data], timeout = 10)\n\n\n\n\n#res = req.json().get('predict')[0]"
  },
  {
    "objectID": "model.html#close-database",
    "href": "model.html#close-database",
    "title": "Model",
    "section": "Close Database",
    "text": "Close Database\n\ncon.close()"
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "api",
    "section": "",
    "text": "import pins\nfrom vetiver import VetiverModel\nfrom vetiver import VetiverAPI\nimport vetiver\n\n\nb = pins.board_folder('data/model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\n\n#Do not run again!!!\n#vetiver.prepare_docker(b, \"penguin_model\")\n\n\napp = VetiverAPI(v, check_prototype=True)\n\n\n #app.run(port = 8080)"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Penguins EDA",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dbplyr)\nlibrary(DBI)\nlibrary(duckdb)"
  },
  {
    "objectID": "eda.html#import-libraries",
    "href": "eda.html#import-libraries",
    "title": "Penguins EDA",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dbplyr)\nlibrary(DBI)\nlibrary(duckdb)"
  },
  {
    "objectID": "eda.html#database-initialization-and-connection",
    "href": "eda.html#database-initialization-and-connection",
    "title": "Penguins EDA",
    "section": "Database Initialization and Connection",
    "text": "Database Initialization and Connection\n\n#con &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"my-db.duckdb\")\n#DBI::dbWriteTable(con, \"penguins\", palmerpenguins::penguins)\n#DBI::dbDisconnect(con)\n\n\ncon &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"my-db.duckdb\"\n  )\ndf &lt;- dplyr::tbl(con, \"penguins\")"
  },
  {
    "objectID": "eda.html#set-up-dataframe",
    "href": "eda.html#set-up-dataframe",
    "title": "Penguins EDA",
    "section": "Set up Dataframe",
    "text": "Set up Dataframe\n\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n        ends_with(\"mm\") | ends_with(\"g\"),\n      \\(x) mean(x, na.rm = TRUE)\n      ),.groups = \"drop\"\n    ) %&gt;%\n  dplyr::collect() %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nsex\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nAdelie\nNA\n37.84000\n18.32000\n185.6000\n3540.000\n\n\nAdelie\nfemale\n37.25753\n17.62192\n187.7945\n3368.836\n\n\nAdelie\nmale\n40.39041\n19.07260\n192.4110\n4043.493\n\n\nChinstrap\nfemale\n46.57353\n17.58824\n191.7353\n3527.206\n\n\nChinstrap\nmale\n51.09412\n19.25294\n199.9118\n3938.971\n\n\nGentoo\nNA\n45.62500\n14.55000\n215.7500\n4587.500\n\n\nGentoo\nfemale\n45.56379\n14.23793\n212.7069\n4679.741\n\n\nGentoo\nmale\n49.47377\n15.71803\n221.5410\n5484.836"
  },
  {
    "objectID": "eda.html#eda-plot",
    "href": "eda.html#eda-plot",
    "title": "Penguins EDA",
    "section": "EDA Plot",
    "text": "EDA Plot\n\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "eda.html#database-disconnect",
    "href": "eda.html#database-disconnect",
    "title": "Penguins EDA",
    "section": "Database Disconnect",
    "text": "Database Disconnect\n\nDBI::dbDisconnect(con, shutdown = T)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "C3C William Jameson Math 378 Final Project",
    "section": "",
    "text": "This project follows Alex K Gold’s book DevOps for Data Science, found at do4ds.com.\nAnalysis was conducted on the Palmer’s Penguins Dataset, cited below.\n(Horst, Hill, and Gorman 2020)"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "C3C William Jameson Math 378 Final Project",
    "section": "",
    "text": "This project follows Alex K Gold’s book DevOps for Data Science, found at do4ds.com.\nAnalysis was conducted on the Palmer’s Penguins Dataset, cited below.\n(Horst, Hill, and Gorman 2020)"
  }
]